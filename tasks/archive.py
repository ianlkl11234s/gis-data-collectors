"""
æ­¸æª”ä»»å‹™

è² è²¬å°‡æœ¬åœ°è³‡æ–™åŒæ­¥åˆ° S3ï¼Œä¸¦æ¸…ç†éæœŸçš„æœ¬åœ°è³‡æ–™ã€‚
"""

import gc
import shutil
from datetime import datetime, timedelta
from pathlib import Path

import config


class ArchiveTask:
    """æ­¸æª”ä»»å‹™ç®¡ç†å™¨"""

    def __init__(self):
        self.s3 = None
        self._init_s3()

    def _init_s3(self):
        """åˆå§‹åŒ– S3 å„²å­˜"""
        if not config.S3_BUCKET:
            print("âš ï¸  S3_BUCKET æœªè¨­å®šï¼Œæ­¸æª”åŠŸèƒ½åœç”¨")
            return

        try:
            from storage.s3 import S3Storage
            self.s3 = S3Storage()
            print(f"âœ“ S3 å„²å­˜å·²é€£æ¥: {config.S3_BUCKET}")
        except Exception as e:
            print(f"âœ— S3 å„²å­˜åˆå§‹åŒ–å¤±æ•—: {e}")
            self.s3 = None

    def run(self):
        """åŸ·è¡Œæ­¸æª”ä»»å‹™"""
        if not config.ARCHIVE_ENABLED:
            print("âš ï¸  æ­¸æª”åŠŸèƒ½å·²åœç”¨ (ARCHIVE_ENABLED=false)")
            return

        if not self.s3:
            print("âš ï¸  S3 æœªè¨­å®šï¼Œè·³éæ­¸æª”")
            return

        print(f"\n{'=' * 60}")
        print(f"ğŸ“¦ é–‹å§‹æ­¸æª”ä»»å‹™")
        print(f"{'=' * 60}")
        print(f"   æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"   ä¿ç•™å¤©æ•¸: {config.ARCHIVE_RETENTION_DAYS}")
        print(f"   S3 Bucket: {config.S3_BUCKET}")

        # æ­¥é©Ÿ 1: åŒæ­¥æ‰€æœ‰è³‡æ–™åˆ° S3
        sync_stats = self._sync_to_s3()

        # æ­¥é©Ÿ 2: æ¸…ç†éæœŸçš„æœ¬åœ°è³‡æ–™
        cleanup_stats = self._cleanup_local()

        # è§¸ç™¼ GC
        gc.collect()

        print(f"\n{'=' * 60}")
        print(f"ğŸ“Š æ­¸æª”å®Œæˆ")
        print(f"{'=' * 60}")
        print(f"   åŒæ­¥: ä¸Šå‚³ {sync_stats['uploaded']} | è·³é {sync_stats['skipped']} | å¤±æ•— {sync_stats['failed']}")
        print(f"   æ¸…ç†: åˆªé™¤ {cleanup_stats['deleted']} å€‹æª”æ¡ˆ | ä¿ç•™ {cleanup_stats['kept']} å€‹æª”æ¡ˆ")
        print(f"{'=' * 60}")

        return {
            'sync': sync_stats,
            'cleanup': cleanup_stats
        }

    def _sync_to_s3(self) -> dict:
        """åŒæ­¥æœ¬åœ°è³‡æ–™åˆ° S3"""
        print(f"\nğŸ“¤ åŒæ­¥è³‡æ–™åˆ° S3...")

        total_stats = {'uploaded': 0, 'skipped': 0, 'failed': 0}

        if not config.LOCAL_DATA_DIR.exists():
            print("   âš ï¸  æœ¬åœ°è³‡æ–™ç›®éŒ„ä¸å­˜åœ¨")
            return total_stats

        # éæ­·æ‰€æœ‰æ”¶é›†å™¨ç›®éŒ„
        for collector_dir in config.LOCAL_DATA_DIR.iterdir():
            if not collector_dir.is_dir():
                continue

            collector_name = collector_dir.name

            # åŒæ­¥è©²æ”¶é›†å™¨çš„è³‡æ–™
            stats = self.s3.sync_directory(
                local_dir=collector_dir,
                s3_prefix=collector_name,
                skip_existing=True
            )

            total_stats['uploaded'] += stats['uploaded']
            total_stats['skipped'] += stats['skipped']
            total_stats['failed'] += stats['failed']

            if stats['uploaded'] > 0:
                print(f"   âœ“ {collector_name}: ä¸Šå‚³ {stats['uploaded']} å€‹æª”æ¡ˆ")
            elif stats['skipped'] > 0:
                print(f"   - {collector_name}: å·²åŒæ­¥ ({stats['skipped']} å€‹æª”æ¡ˆ)")

        return total_stats

    def _cleanup_local(self) -> dict:
        """æ¸…ç†éæœŸçš„æœ¬åœ°è³‡æ–™"""
        print(f"\nğŸ—‘ï¸  æ¸…ç†éæœŸè³‡æ–™ (>{config.ARCHIVE_RETENTION_DAYS} å¤©)...")

        stats = {'deleted': 0, 'kept': 0}
        cutoff_date = datetime.now() - timedelta(days=config.ARCHIVE_RETENTION_DAYS)

        if not config.LOCAL_DATA_DIR.exists():
            return stats

        # éæ­·æ‰€æœ‰æ”¶é›†å™¨ç›®éŒ„
        for collector_dir in config.LOCAL_DATA_DIR.iterdir():
            if not collector_dir.is_dir():
                continue

            collector_name = collector_dir.name

            # éæ­·å¹´/æœˆ/æ—¥ç›®éŒ„çµæ§‹
            deleted_count = 0
            for json_file in collector_dir.glob('**/*.json'):
                # è·³é latest.json
                if json_file.name == 'latest.json':
                    stats['kept'] += 1
                    continue

                # æª¢æŸ¥æª”æ¡ˆä¿®æ”¹æ™‚é–“
                file_mtime = datetime.fromtimestamp(json_file.stat().st_mtime)

                if file_mtime < cutoff_date:
                    # ç¢ºèª S3 ä¸Šå·²æœ‰æ­¤æª”æ¡ˆ
                    rel_path = json_file.relative_to(collector_dir)
                    s3_key = f"{collector_name}/{rel_path}"

                    if self.s3.file_exists(s3_key):
                        json_file.unlink()
                        deleted_count += 1
                        stats['deleted'] += 1
                    else:
                        stats['kept'] += 1
                else:
                    stats['kept'] += 1

            if deleted_count > 0:
                print(f"   âœ“ {collector_name}: åˆªé™¤ {deleted_count} å€‹éæœŸæª”æ¡ˆ")

                # æ¸…ç†ç©ºç›®éŒ„
                self._cleanup_empty_dirs(collector_dir)

        return stats

    def _cleanup_empty_dirs(self, base_dir: Path):
        """æ¸…ç†ç©ºç›®éŒ„"""
        for dir_path in sorted(base_dir.glob('**/*'), reverse=True):
            if dir_path.is_dir() and not any(dir_path.iterdir()):
                dir_path.rmdir()

    def get_archive_status(self) -> dict:
        """å–å¾—æ­¸æª”ç‹€æ…‹"""
        status = {
            'enabled': config.ARCHIVE_ENABLED,
            's3_configured': self.s3 is not None,
            's3_bucket': config.S3_BUCKET,
            'retention_days': config.ARCHIVE_RETENTION_DAYS,
            'archive_time': config.ARCHIVE_TIME,
            'local_data_dir': str(config.LOCAL_DATA_DIR),
            'collectors': []
        }

        if not config.LOCAL_DATA_DIR.exists():
            return status

        # çµ±è¨ˆå„æ”¶é›†å™¨çš„è³‡æ–™
        for collector_dir in config.LOCAL_DATA_DIR.iterdir():
            if not collector_dir.is_dir():
                continue

            files = list(collector_dir.glob('**/*.json'))
            files = [f for f in files if f.name != 'latest.json']

            total_size = sum(f.stat().st_size for f in files)

            collector_status = {
                'name': collector_dir.name,
                'local_files': len(files),
                'local_size_mb': round(total_size / (1024 * 1024), 2)
            }

            # å¦‚æœ S3 å¯ç”¨ï¼Œçµ±è¨ˆ S3 ä¸Šçš„è³‡æ–™
            if self.s3:
                s3_files = self.s3.list_files(collector_dir.name)
                s3_size = sum(f['size'] for f in s3_files)
                collector_status['s3_files'] = len(s3_files)
                collector_status['s3_size_mb'] = round(s3_size / (1024 * 1024), 2)

            status['collectors'].append(collector_status)

        return status
